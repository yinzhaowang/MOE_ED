{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hazardous-condition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from mmoe_iso_calibration import MMoE, IsotonicCalibrator, outcome_list, col_list\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create directory for saving threshold plots\n",
    "os.makedirs(\"threshold_plots\", exist_ok=True)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_data_and_model():\n",
    "    \"\"\"Load test data, model, and calibrator\"\"\"\n",
    "    print(\"Loading data, model, and calibrator...\")\n",
    "    \n",
    "    # Load test data (same as in app.py)\n",
    "    df_test = pd.read_csv(\"../../../data/test_with_outcomes.csv\")\n",
    "    raw_feats = df_test.loc[:, col_list]\n",
    "    feat_df = raw_feats.select_dtypes(include=[np.number, bool])\n",
    "    feat_df = feat_df.fillna(feat_df.mean())\n",
    "    X_np = feat_df.to_numpy(dtype=float)\n",
    "    y_np = df_test.loc[:, outcome_list].to_numpy(dtype=float)\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    X_t = torch.from_numpy(X_np).float()\n",
    "    y_t = torch.from_numpy(y_np).float()\n",
    "    \n",
    "    # Create dataloader\n",
    "    batch_size = 256\n",
    "    dataset = TensorDataset(X_t, y_t)\n",
    "    test_dl = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Load model\n",
    "    input_size = X_np.shape[1]\n",
    "    num_experts = 8\n",
    "    expert_hidden = [128, 64]\n",
    "    expert_output_dim = 32\n",
    "    tower_hidden_dim = 16\n",
    "    task_output_dims = [1] * len(outcome_list)\n",
    "    \n",
    "    model = MMoE(input_size, num_experts, expert_hidden, expert_output_dim, \n",
    "                tower_hidden_dim, task_output_dims).to(device)\n",
    "    model.load_state_dict(torch.load(\"best_mmoe_iso.pt\", map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Load calibrator\n",
    "    calibrator = joblib.load(\"calibrator.pkl\")\n",
    "    \n",
    "    return df_test, X_np, y_np, model, calibrator, test_dl\n",
    "\n",
    "def get_calibrated_predictions(model, test_dl, calibrator):\n",
    "    \"\"\"Get calibrated predictions for all test data\"\"\"\n",
    "    print(\"Generating calibrated predictions...\")\n",
    "    \n",
    "    all_logits = []\n",
    "    all_targets = []\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(test_dl):\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits_list = model(X_batch)\n",
    "            logits = torch.cat(logits_list, dim=1).cpu().numpy()\n",
    "            all_logits.append(logits)\n",
    "            all_targets.append(y_batch.numpy())\n",
    "    \n",
    "    # Concatenate batches\n",
    "    all_logits = np.vstack(all_logits)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "    \n",
    "    # Apply calibration\n",
    "    calibrated_probs = calibrator.predict(all_logits)\n",
    "    \n",
    "    return calibrated_probs, all_targets\n",
    "\n",
    "def find_optimal_thresholds(calibrated_probs, y_true):\n",
    "    \"\"\"Find optimal threshold for each outcome by maximizing Youden's J statistic (sensitivity + specificity - 1)\"\"\"\n",
    "    print(\"Finding optimal thresholds...\")\n",
    "    \n",
    "    optimal_thresholds = {}\n",
    "    threshold_metrics = {}\n",
    "    \n",
    "    for i, outcome in enumerate(outcome_list):\n",
    "        # Get actual outcomes and predicted probabilities for this disease\n",
    "        y_true_outcome = y_true[:, i]\n",
    "        y_prob_outcome = calibrated_probs[:, i]\n",
    "        \n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_true_outcome, y_prob_outcome)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Calculate Youden's J statistic (sensitivity + specificity - 1)\n",
    "        j_scores = tpr - fpr\n",
    "        optimal_idx = np.argmax(j_scores)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        \n",
    "        # Calculate precision-recall curve\n",
    "        precision, recall, pr_thresholds = precision_recall_curve(y_true_outcome, y_prob_outcome)\n",
    "        pr_auc = average_precision_score(y_true_outcome, y_prob_outcome)\n",
    "        \n",
    "        # Store results\n",
    "        disease_name = outcome.replace(\"outcome_\", \"\")\n",
    "        optimal_thresholds[disease_name] = optimal_threshold\n",
    "        \n",
    "        # Store additional metrics\n",
    "        threshold_metrics[disease_name] = {\n",
    "            'threshold': optimal_threshold,\n",
    "            'sensitivity': tpr[optimal_idx],\n",
    "            'specificity': 1 - fpr[optimal_idx],\n",
    "            'youden_j': j_scores[optimal_idx],\n",
    "            'prevalence': np.mean(y_true_outcome),\n",
    "            'roc_auc': roc_auc,\n",
    "            'pr_auc': pr_auc\n",
    "        }\n",
    "        \n",
    "        # Create and save plots\n",
    "        plot_threshold_curves(disease_name, fpr, tpr, roc_auc, precision, recall, \n",
    "                             pr_auc, optimal_threshold, y_true_outcome, y_prob_outcome)\n",
    "    \n",
    "    return optimal_thresholds, threshold_metrics\n",
    "\n",
    "def plot_threshold_curves(disease_name, fpr, tpr, roc_auc, precision, recall, \n",
    "                         pr_auc, optimal_threshold, y_true, y_prob):\n",
    "    \"\"\"Create and save ROC and PR curves with optimal threshold marked\"\"\"\n",
    "    \n",
    "    # Set up the figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    ax1.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    \n",
    "    # Mark the optimal threshold on ROC curve\n",
    "    optimal_idx = np.argmin(np.abs(np.array([x for x in fpr]) - (1 - np.array([x for x in tpr]))))\n",
    "    ax1.plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', markersize=8, \n",
    "            label=f'Optimal threshold = {optimal_threshold:.3f}')\n",
    "    \n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "    ax1.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "    ax1.set_title(f'ROC Curve for {disease_name}')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Plot PR curve\n",
    "    ax2.plot(recall, precision, lw=2, label=f'PR curve (AUC = {pr_auc:.3f})')\n",
    "    ax2.axhline(y=np.mean(y_true), color='k', linestyle='--', \n",
    "               label=f'Baseline (prevalence = {np.mean(y_true):.3f})')\n",
    "    \n",
    "    # Mark the optimal threshold on PR curve\n",
    "    threshold_indices = np.digitize(optimal_threshold, [0.5])\n",
    "    if len(precision) > threshold_indices:\n",
    "        pr_idx = min(threshold_indices, len(precision)-1)\n",
    "        ax2.plot(recall[pr_idx], precision[pr_idx], 'ro', markersize=8,\n",
    "                label=f'Optimal threshold = {optimal_threshold:.3f}')\n",
    "    \n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title(f'Precision-Recall Curve for {disease_name}')\n",
    "    ax2.legend(loc=\"lower left\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"threshold_plots/{disease_name}_threshold_curves.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_threshold_heatmap(threshold_metrics):\n",
    "    \"\"\"Create a heatmap of threshold metrics across all diseases\"\"\"\n",
    "    \n",
    "    # Create a DataFrame for the heatmap\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Disease': [],\n",
    "        'Threshold': [],\n",
    "        'Prevalence': [],\n",
    "        'Sensitivity': [],\n",
    "        'Specificity': [],\n",
    "        'Youden J': [],\n",
    "        'ROC AUC': [],\n",
    "        'PR AUC': []\n",
    "    })\n",
    "    \n",
    "    # Fill the DataFrame with metrics\n",
    "    for disease, metrics in threshold_metrics.items():\n",
    "        metrics_df = metrics_df.append({\n",
    "            'Disease': disease,\n",
    "            'Threshold': round(metrics['threshold'], 3),\n",
    "            'Prevalence': round(metrics['prevalence'], 3),\n",
    "            'Sensitivity': round(metrics['sensitivity'], 3),\n",
    "            'Specificity': round(metrics['specificity'], 3),\n",
    "            'Youden J': round(metrics['youden_j'], 3),\n",
    "            'ROC AUC': round(metrics['roc_auc'], 3),\n",
    "            'PR AUC': round(metrics['pr_auc'], 3)\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    # Sort by AUC\n",
    "    metrics_df = metrics_df.sort_values('ROC AUC', ascending=False)\n",
    "    \n",
    "    # Save the metrics to CSV\n",
    "    metrics_df.to_csv(\"threshold_metrics.csv\", index=False)\n",
    "    print(f\"Saved threshold metrics to threshold_metrics.csv\")\n",
    "    \n",
    "    # Create heatmap visualization\n",
    "    plt.figure(figsize=(15, len(threshold_metrics)*0.6))\n",
    "    \n",
    "    # Create a heatmap for numeric columns only\n",
    "    heatmap_data = metrics_df.set_index('Disease')\n",
    "    heatmap_data = heatmap_data.astype(float)\n",
    "    \n",
    "    # Create custom colormap\n",
    "    cmap = sns.diverging_palette(240, 10, as_cmap=True)\n",
    "    \n",
    "    # Create heatmap\n",
    "    ax = sns.heatmap(heatmap_data, annot=True, fmt=\".3f\", cmap=cmap, \n",
    "                    linewidths=0.5, cbar=True)\n",
    "    plt.title('Optimal Thresholds and Performance Metrics by Disease')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"threshold_plots/threshold_metrics_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "defined-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Main function to find optimal thresholds\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"OPTIMAL THRESHOLD CALCULATOR\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load data and model\n",
    "    df_test, X_np, y_np, model, calibrator, test_dl = load_data_and_model()\n",
    "    \n",
    "    # Get calibrated predictions\n",
    "    calibrated_probs, y_true = get_calibrated_predictions(model, test_dl, calibrator)\n",
    "    \n",
    "    # Find optimal thresholds\n",
    "    optimal_thresholds, threshold_metrics = find_optimal_thresholds(calibrated_probs, y_true)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nOptimal Thresholds:\")\n",
    "    print(\"-\" * 80)\n",
    "    for disease, threshold in sorted(optimal_thresholds.items(), \n",
    "                                   key=lambda x: threshold_metrics[x[0]]['roc_auc'], \n",
    "                                   reverse=True):\n",
    "        auc = threshold_metrics[disease]['roc_auc']\n",
    "        sensitivity = threshold_metrics[disease]['sensitivity']\n",
    "        specificity = threshold_metrics[disease]['specificity']\n",
    "        prevalence = threshold_metrics[disease]['prevalence']\n",
    "        \n",
    "        print(f\"{disease.ljust(20)}: {threshold:.3f} (AUC: {auc:.3f}, \"\n",
    "              f\"Sens: {sensitivity:.3f}, Spec: {specificity:.3f}, Prev: {prevalence:.3f})\")\n",
    "    \n",
    "    # Create heatmap and save metrics\n",
    "    create_threshold_heatmap(threshold_metrics)\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Threshold plots saved to the 'threshold_plots' directory.\")\n",
    "    print(\"Threshold metrics saved to 'threshold_metrics.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-wholesale",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 py_ml_dl",
   "language": "python",
   "name": "py_ml_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
